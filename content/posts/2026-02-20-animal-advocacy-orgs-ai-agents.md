---
title: "You are letting animals die by missing out on AI productivity"
date: 2026-02-20
draft: false
---

## Advice for animal advocacy orgs and job seekers in the age of agents

### In this post:

- **AI agents have arrived,** and animal advocacy organizations that don't adopt them immediately will fall behind– with animals bearing the cost.

- Claude Code gives every activist the power of a software engineer; Claude Cowork extends it to all knowledge work. **Every activist should be using Claude Cowork.**

- Agents like OpenClaw cross a new threshold: full AI assistants that can access all your tools and don't wait for instructions. **Organizations should rush to adopt agents as soon as security concerns are addressed.**

- In this new era, organizations will have exactly two roles: **agent orchestrators** who automate vast digital work, enabling **social adepts** who focus purely on relationships. Pick a lane and start specializing.

- Small orgs should stop defining themselves by a single intervention; **focus instead on building an agile vehicle** that can jump on strategic opportunities as they appear.

- Big orgs need to **clear away stone-age IT policies** that are preventing their most ambitious team members from harnessing the most powerful AI tools.

---

## Takeoff is present-tense

I arrived in San Francisco two weeks ago for the Sentient Futures Summit, a gathering at the intersection of animal movement strategy and artificial intelligence. About 150 people spent the weekend (and most of the following week) getting down into the weeds discussing how to ensure the arrival of superintelligent AI leads to less animal cruelty rather than more.

I was blown away by the talent density at the conference. Many people have been thinking deeply about how to make this go well. In one sense, it made me feel more hopeful. But it also drove home just how much work remains to be done.

For my first report back from the conference, I'm going to focus on **what the arrival of agentic AI means for the internal workings of advocacy organizations.**

I've been wondering what effective advocacy organizations will look like in a world where most intellectual work can be automated. In the last two weeks, the answer snapped into focus. This was partly because of being surrounded by people on the bleeding edge of AI adoption, and partly because of developments at that bleeding edge that happened just days before the summit.

Everything in this post is both a prediction and a dare. Within one year, the most effective organizations will be working the way I describe below. But more importantly, _you could start implementing all of this in your organization this week._ Yes, there will be a learning curve and some upfront cost. But the sooner you start, the sooner you will reap the benefits. And since all those benefits pass on directly to the animals you fight for, you can't afford to wait.

We'll break it into three parts:

1. Agents are here, and if you (yes, _you_) are not using them, you're letting animals die.

2. Organizations in the agent era have exactly two roles for humans.

3. Small orgs need to think big, and big orgs need to act small.

The first point is for everyone. The third is most important for leaders. The second is somewhere in between.

---

## 1. In glorious AI future, $20 per month wastes you

First, the bad news: if you are still using LLMs through a website like chatgpt.com or claude.ai, you have fallen terribly behind. That's, like, so 2024. It's like having a Myspace.

Fortunately, you can still catch up. In a day's work, you could become one of the top 2% of AI power users.

Let me quickly catch you up on what happened while you were distracted by normal things. It might seem like a history lesson. But to understand what the current tools can do, it helps to understand the incremental way these capabilities piled up.

### 1.1 The year of agents

One year ago, everyone was excited about generative AI. But AI was synonymous with chat bots. You could log onto ChatGPT or Claude through their website interface, ask a question, get an answer, and get into a back-and-forth with the model.

Chatbots were (and are) genuinely helpful. They give feedback on your writing, help you think through strategic questions, and generate pro-quality documents like legal contracts. Yay! They can even make you cool art of a pig in a rocket ship.

All that is nice, like a weirdly knowledgeable friend. But if you were having a hard time seeing how these chatbots posed a threat to the economic usefulness of humans, I couldn't blame you. Back in February 2025, they couldn't figure out how to click buttons on the most basic web page.

Over 12 months, a sequential series of breakthroughs has changed everything:

- **Deep Research (Feb 2025) –** first released on ChatGPT, this allowed LLMs to break out of their vast but limited inherent knowledge to effectively pull in any publicly searchable info on the internet. It was the first real-world scalable example of _tool use_– despite their digital nature, searching the internet is no more inherent for LLMs than for humans, and it requires learning to use an outside tool. Finally, deep research was a sharp expansion in the _task length_ available to consumers, who learned that—just like a human—GPT could come up with much better answers if it had 15 minutes to think and do research. In those 15 minutes, GPT prepares reports that would take a competent researcher a day or more, though still only based on searching existing information.

- **Claude Code (limited release Feb 2025, full rollout in May) –** Claude Code (CC) was a much bigger watershed moment, though at first it went unnoticed outside of software engineers. CC was not a new AI model. It was an environment for Claude to operate in, freeing it from the shackles of a web browser and giving it the ability to read, create, and edit files on your own computer. (The actual LLM still runs in the cloud and requires an internet connection.) Suddenly you could collaborate on documents directly with Claude. Once Claude's "context window" (i.e short-term memory) fills up, you can start a new session, Claude quickly gets up to speed on its own previous work, and you're off to the races. This created a native environment for Claude to use Claude-shaped tools, but it still got lobotomized after an hour or two of work.

- **Zapier adopts the Model Context Protocol (April 2025) –** Key infrastructural layers of the internet open themselves up directly to LLMs. While ChatGPT and Claude were still struggling to navigate a web page the way humans do, MCP gave them a more direct way to access internet software. CAPTCHA exists to stop bots from using the internet gateways meant for humans, but now a new doorway was being opened specifically for LLMs.

- **The frontier goes agentic (November 2025) –** Throughout 2025, the digital environment was being reorganized to accommodate autonomous LLM agents. The thing that was missing was agentic intelligence in the models themselves. Finally, in one dizzying week in November, they arrived: Gemini 3, Claude Opus 4.5, and GPT-5.1-Codex-Max each represented a huge leap forward in autonomy. Inside coding harnesses like Claude Code, these agents could work autonomously for hours on end. From a single prompt describing a new app, they could outline requirements, make a plan, complete engineering tasks one after another, test the product, and bring it live on the internet, all without human oversight if you chose to trust them.

- **Claude Cowork (January 2026) –** With Claude Code exploding in popularity, Anthropic realized the name and command-line interface were needlessly turning away non-technical users. CC could be used for collaborating on any document; it could even use the internet through your own browser sessions, logging into accounts as you. So in a matter of days, they tasked Claude with creating a more user-friendly interface, like Claude Code for dummies. The result was Claude Cowork, a desktop app anyone can easily download and start using in minutes for collaboration on any computer task.

- **OpenClaw (January 2026) –** and the world without agents was nevermore.

When I first heard Claude Cowork was about to be released, I made a note to myself to blast out a quick post called "If you're not using AI this way, you're letting animals die." Even as a non-coder, I'd already switched to using Claude Code for most tasks. But the code interface was slightly annoying, so I downloaded Cowork on day one. Despite some initial bugs, I could see a fuzzy picture taking shape. This, I thought, was the future of work.

But before I could write a word of the post, my picture was torn to pieces by a lobster-like appendage.

### 1.2 Snap goes the claw

I'm not going to rehash the entire saga and ensuing hilarity of how Clawdbot (a bored engineer's ironic side project) turned into Moltbot and designed itself Moltbook, the first social media site for AI agents to socialize with each other directly. It's not that it's not worth reading– besides being world-historically fascinating, Moltbook can actually go a long way to understanding what OpenClaw is and where it might take us. Scott Alexander already covered this and you should absolutely read his account if you consider yourself a curious person.

Instead, I'm going to focus on what OpenClaw means for work. I'd call it the future of work, but it's already here. OpenClaw is the present of work, and if you aren't using it, you're stuck in the past.

OK, that's a _slight_ overstatement, for exactly one reason: OpenClaw is version one, and it's very messy. Users who don't understand digital security are exposing themselves to considerable risks. If software offerings were to freeze in place on Feb 18, 2026, then I would tell you to overcome those risks, by having Claude teach you enough about cybersecurity to use OpenClaw safely. Instead, you can wait a few days or something for like 10 secure versions of OpenClaw to land on the market. One such was just announced with a $5M Superbowl ad after spending $70M on the most expensive domain acquisition in history. That one seems sus but just wait a couple more days.

With that warning aside, let's see what OpenClaw has to tell us about the futurepresent of work.

OpenClaw is a personalizable AI agent that runs on a computer you control. The intelligence is still powered though an LLM in the cloud like Claude or ChatGPT, but the agent itself is no longer passive. Where even Claude Code would only work when you told it to, OpenClaw can initiate tasks on your behalf. Every thirty minutes, it wakes itself up, reviews a customizable to-do list of recurring tasks, and checks to see whether there's any new work to be done. (Of course, it's also always ready when you want.)

Claude Code made it possible for any schmuck to act like a mid-level software engineer. Claude Cowork enables any knowledge worker to work faster, and lifts up your weakest skills.

But OpenClaw is fundamentally different. It crosses a threshold from speeding you up to acting like a separate person. It is like having an executive assistant, except that your assistant is also a world-class software engineer, an excellent strategist, and a very competent writer with encyclopedic knowledge. And if you want, you can have more than one. Ten, one hundred– as many as you can put to use. Most importantly, you don't need _any_ technical skill to set all this up. You just tell OpenClaw what you want, in plain English, and it configures itself.

If you choose to give it access, OpenClaw can do everything on your computer that you can do. _Everything._ Read and answer emails, modify your calendar, write google docs, schedule meetings, listen to those meetings and take notes, then summarize those notes for you a few minutes before you meet with that person again.

If you're forward-thinking on AI, you may have already automated some of these. Several of my friends use an AI tool called Fyxer, which sorts their email inbox into categories, produces a daily summary, and drafts replies to emails that seem to need one. But OpenClaw (or a successor) will swallow Fyxer up along with all its brethren. The reason is that, while Fyxer is specialized to a specific domain, your OpenClaw agent is learning about you from across your entire professional life. Fyxer creates a generic email reply, but OpenClaw can create a reply based on everything it knows about you.

Of course, it can also write code for you, automating any task you'd do more than three times. If you're willing to spend on the tokens, your OpenClaw agents can build enterprise-scale codebases in a matter of days. If you've been feeling annoyed with some missing feature in a piece of software your team uses, OpenClaw can just rebuild the whole thing, creating a custom version for exactly what your organization needs. If you learn to be clear about exactly what you need, it has a good chance of building small apps in one shot.

You can talk to OpenClaw anywhere. Give it a phone number and text it through Signal or Whatsapp– or give it access to your phone number and let it text other people on your behalf. You can have it give you a phone call every morning to tell you about the days' headlines, and answer whatever questions you have. You can do all of the above, and in every case, you'll be talking to the same agent, with consistent, coherent memory across the board.

All of this is _here right now._ This _is_ the drop-in AI knowledge worker. It won't feel like an expert in you on day one– but then, neither would a human employee. And if it would take six months for your human executive assistant to really learn the ins and outs of working with you, well… with a little imagination, I'm confident you can start to imagine what these systems will be capable of in six months.

Downloading OpenClaw is free, as is spinning up an arbitrary number of agents. Actually having them do tasks is pay-as-you-go through an API key from your preferred LLM provider. (If you really want to be a power user, set yourself up with the ability to use different LLMs for different tasks, saving on simpler tasks with cheaper models.) If you start using agents like this seriously, you should expect to spend much more than $20 a month, the standard tier for ChatGPT and Claude accounts.

Here's the thing: if you are a knowledge worker in the animal movement in 2026 and you are only paying $20 a month for inference, with all due respect, _that is straight up dumb._

The power user tier for both ChatGPT and Claude costs $200 per month, or $2,400 per year. That's far less than 10% of the salary for the lowest-paid full time employee. It's less than 5% at most organizations. Can most employees get a greater than 10% uplift in their productivity with AI agents in 2026? If you think the answer is no, you deserve to fail. If you lead an organization, you should be insisting that every single one of your employees be maxxing out a $200/month Claude Max plan. If someone manages to rack up a $20,000 bill for API tokens using OpenClaw, give them employee of the month. (More on this in part 3.)

Am I being hyperbolic? Only in the sense that there are unfortunately ineffective ways to spend OpenClaw tokens. Yes, the focus should be somewhat on using agents effectively. But at this juncture, the most effective thing to do is learn and make mistakes. $1,000 of tokens wasted on a silly project is the most valuable professional development expenditure on the market in 2026. So honestly, go waste some tokens.

---

## 2. Are you a John or a Rachel?

OpenClaw went viral in the week leading up to the Sentient Futures Summit. I could now see what it will look like to work with AI agents. But I didn't yet see what that meant for organizations.

One conversation cleared everything up. I was sitting in a circle with several friends who wanted to learn how to use the latest AI tools, especially OpenClaw. Class was in session, and the instructor was a disheveled John Boland from Phauna Foundation, just emerging from a dayslong vibecoding bender that had forever changed how he would see the world.

### 2.1 The agent orchestrator

A few days before John traveled to San Francisco for the summit, Phauna Foundation had announced that they were hiring an operations manager, someone to handle the administrative work of running a philanthropic foundation: organizing the google drive, tracking grants, bookkeeping, notetaking for meetings, you get the picture.

But the day he was traveling, he discovered OpenClaw. He set up an OpenClaw agent on a secure machine and started playing around. As he tested out the agent's capabilities, it started to dawn on him that he could automate the role instead. OpenClaw could access all the tools an ops manager would need, and excelled at most of the tasks.

Once he truly entertained the possibility of automating a full role, John's ambitions climbed rapidly. Eventually, when he ran into trouble trying to integrate two pieces of software that didn't want to talk to each other, he thought, _why not just rebuild them both?_

As he got more of a feel for OpenClaw, he learned what others before him had realized about the convenience of spinning up different agents, with different specialties. Soon he had three specialist agents loaded up with different skills, overseen by a fourth, a generalist with a view of the whole project.

After five days pounding away at his laptop in a corner of the conference venue, John hadn't just built an AI operations manager for Phauna. He had completely reengineered the functionality of Google drive, Asana, Airtable, Quickbooks, and several other SAAS products into a single unified, AI-first foundation management platform.

John's speed and obsessive drive are… not typical. But the final product might be: when Anthropic released Claude Code in January, it triggered a $300 billion selloff of enterprise software stocks like Salesforce and Intuit (maker of Quickbooks). Investors realized that these software products are just an inefficient way for humans to try to make use of digital information, and agents will be much more efficient.

In that moment, I could see what it will look like for John to operate at peak efficiency. One person at a workstation facing a grid of six monitors, each one connecting to a different agent itself overseeing a swarm of specialized subagents. Each swarm could be fulfilling a completely different aspect of an organizations' work: one is researching campaign targets, another overseeing a digital ad campaign, a third planning and promoting a protest or conference, and the others taking care of the organization's administrative needs. Sometimes they might build software, but for the most part, software as we understand it would be obsolete, with digital intelligence accessing and manipulating data on a case-by-case basis.

Even as someone who has long been bullish about the chance that AI will fundamentally transform the world, seeing John actually bring this to life filled me with several flavors of awe. Sure, he rattled off costly lessons as a warning. As the project grew more complicated, agents started breaking things more and more, in ways that could cause serious problems for an organization. He burned through about $600 in API tokens over a weekend, and while he had built a lot of software, none of it was quite ready for deployment. But it didn't take much imagination to see how soon these struggles would be solved by smarter AI models and better agent harnesses.

### 2.2 The social adept

Awe was not the uniform reaction of everyone sitting in that circle, however. To another attendee, this all sounded like a pile of trouble. It was Rachel Atcheson, one of the animal movement's top political operatives. Before founding Food Policy Pathways to get more food system reformers inside the government, Rachel worked in the Eric Adams administration bringing plant-based defaults to New York City hospitals.

Politics, of course, is all about relationships. It is perhaps the line of work least similar to software engineering, and when she's lobbying, Rachel is surrounded by technophobes– an internal poll found 95% of members of Congress have never used an AI tool like ChatGPT, either for work or personal use.

The vision of work I was getting from John made Rachel groan. She's never going to become an AI power user like that– and she shouldn't! It would be a total waste of her social skills, which by all indications will be the last domain in which AI can replace humans.

That's the key point: as AI supercharges the computer savvy and allows them to automate the work that happens in front of a computer, the nature of advocacy organizations will change. It will distill down to the point where every organization has exactly two kinds of roles.

One role is hyperscaling AI orchestration, where a single power user with access to enough compute can carry out work that used to take dozens or hundreds of people.

That leaves the rest of an organization's team to focus on interfacing with humans in the physical world. Social skill will become the bottleneck to scaling every animal advocacy strategy: lobbying politicians, negotiating with corporate executives, organizing volunteers to hold protests, building alliances across society, and managing human employees.

We'll need an army of Rachels. And with Johns behind them, the Rachels will be able to specialize like never before. I mean it: once we get our agents together, _Rachel will never have to open a laptop again._ She'll interface with her personal agents on mobile through text messages– or if she prefers, voice notes and phone calls. They'll optimize her calendar, providing her with exactly the information she needs for her next meeting based on notes they took in all her previous meetings. Liberated from touching a keyboard, she can get maximum mileage out of her legendary networking skills.

### 2.3 Make yourself useful

I've been known to get excited about things from time to time here on Sandcastles. So let me clarify that I don't quite expect _all_ computer-based work to reduce to orchestrating agents in 2026. Naturally, blogging is too hard and special to be ever done by AI. As for the rest of you, I admit there are some high-level creative tasks that LLMs don't seem close to being solved. Strategic planning involves too much context across different domains, and too much of what AI researchers call _tacit knowledge–_ that is, the knowledge experts pick up over a lifetime of experience without ever explicitly stating it to themselves, which therefore never gets written down and fed into LLM training data.

Strategic planning (and yes, even blogging) will fall someday. But already, these are the exception. The large majority of what knowledge workers do at their computers is automatable today, or is in the process of becoming automatable.

I know this might sound scary for individuals employed at advocacy organizations. But it is great for animals. It means we can dramatically multiply our impact even if the funding available for our movement stays static.

I don't see any reason this should decrease the total payroll of the animal movement. But it should raise the bar. The potential impact per person has just increased sharply, and it will continue to increase. It's up to you to make sure your actual impact increases with it.

I recommend choosing either the John path or the Rachel path. But I warn you: _if you are going to try to worm out of the call to action in the first half of this post by claiming you're a Rachel, you better become goddamn extraordinary._

I both expect and hope that the movement will hire more Johns than Rachels in 2026. **If you lead an organization,** I've got great news for you: there are now two years' worth of 23-year-old computer science grads whose college career was defined by the arrival of AI and who just watched the ladder of entry-level software engineering jobs pulled away before they could grab hold. Snatch these people up and put them to work agentifying your organization!

**And job seekers:** if you have a CS degree, that's great, but you don't need one. I went to music school, and if I was looking to get a job in the movement today, this is exactly what I'd do: I'd spend one month holed up in my mom's basement playing with Claude Code, Codex, and OpenClaw for 10 hours a day and becoming ridiculously competent. Then I'd reach out to an organization I admire and tell them I'd like to intern for free for two months. By the end of two months, you could make yourself indispensable to any org in the space and they'll be begging you to come on full time.

**Leaders and job seekers alike,** here's how I'd structure this. I'd have the AI whiz go person by person through the entire org chart, starting near the top (maybe not right at the top because you'll learn and get better after a few rounds.) I'd spend one week with each person, shadowing them, learning about all the repetitive pain points in their job, and building custom agent deployments for them. Once you work through the entire organization, you can start over for a second pass, then settle into a stable rhythm continuously updating your masterpiece and building incredible new tools nobody ever dreamed of.

### 2.4 Caveats

For all but the most extroverted among us, 8 hours of meetings a day probably sounds exhausting. We like to take a meeting here and there, but focused work in front of a computer is a satisfying and productive chance to recharge our social battery.

Is there room for people to split themselves as 50/50 John-Rachel hybrids? Without a doubt, some extremely effective solo operatives will work this way. And there's no strict rule saying someone inside an organization couldn't do it. But it will be hard. You'll have to _excel_ at both skill sets, because the bar will be high. If this calls to you, today is the day to start honing your skills.

Finally, how long will this world last, with organizations comprised of Johns and Rachels? Eventually, AI itself will overtake humans in both roles. Nobody knows how long that will take, and I don't have a very confident guess. But whether it's long or short, I'm willing to bet the farm on what comes after: the equilibrium I'm describing is the last world we will pass through before human labor becomes obsolete.

---

## 3. To be big, or to be small? That is not the question.

Things are about to get really weird. While I think it's important for animal advocacy organizations to get as much juice out of AI tools as possible, ultimately I'm more concerned with our ability to respond to dramatic changes in our strategic environment.

Of course, these two things are not unrelated. Using AI tools effectively could make us more versatile. Also, seeing how organizations adapt to new tools now is a test run for how they'll react to wider disruption later on.

In that light, I've been thinking a lot about the benefits of small organizations. I've spoken with a number of people who are working as consultants to help advocacy organizations implement AI tools. They all report the same thing: the agility of small orgs is shining at this moment. Some small orgs have entirely restructured themselves to make use of the new capacity freed up by AI. Meanwhile, large orgs are getting tripped up in risk aversion and bureaucracy, with outdated policies actively preventing ambitious team members from leveraging tools that are already generations out of date, to say nothing of the bleeding edge stuff.

As a founder of an advocacy startup, I naturally have a soft spot for small organizations. This all fits my priors. So I was surprised to find that a few conversations this week totally shifted my perspective.

While I still cherish the agility of small orgs, I've noticed a key advantage of big orgs that I'd never thought about before: at their best, big orgs are more strategically agnostic.

The problem for small organizations is that their whole identity is usually bound up in a single project. You found an organization to do a specific thing. You get money by selling that idea to funders, and now you feel pressure to deliver what you promised.

But most ideas just don't work out. Look at the for-profit sector. 60% of startups never turn a profit. Of the 40% that do, many of them wind up succeeding with a product completely unrelated to their original concept. The company that makes Slack was originally formed to build a video game, and when their game sucked, they decided to instead sell the internal messaging tool they'd built for themselves.

In a for-profit startup, you can't bullshit your way to success. Eventually, you have to deliver a product people actually want. But in nonprofits, this connection is severed: there's only a tenuous relationship between having a real-world impact and convincing people to give you money. Sometimes these are even inversely correlated, as Effective Altruists love to point out.

This is epistemically corrosive. Naively translating from the for-profit sector, we should expect something like 60% of new advocacy ideas to fail. Nobody is better positioned to notice an idea is failing than the people working on it. Yet the incentive for these people is to ignore these signals, to convince themselves it's all going to work out, because if it doesn't, they might need to shut down the org they've poured years of their lives into building. And they might worry that after this costly failure, nobody will want to trust them again.

What if the same project is launched as a pilot inside of a large organization with an established identity and funding streams? Say an organization of 100+ people sticks two or three trusted team members on the project full time for six months. After six months, there are some promising signs, so they extend it for another six months. But by the end of a year, things are looking bad. The team reports back: _sure, we could keep trying this and there's a chance it would work. But that chance looks smaller than it did when we originally set out._ Together, leaders decide to call the project a failure, and the team members redirect their energy towards a more promising new idea.

Small orgs define themselves around the pursuit of a single strategy. Large orgs instead tend to think of themselves as building an institution. Small orgs tend to be more agile on an _operational_ level, but larger orgs should find it easier to experiment more on the _strategic_ level.

For a real-world example, consider Anima International, a European umbrella group operating with $6M/year and over 100 staff. I first heard of Anima as one of the main orgs responsible for corporate cage-free campaigns in Europe, so I tucked them away into the category in my head for "corporate welfare campaigners." But it turns out Anima doesn't fit in this box at all– they've racked up national political victories that cover the typical abolition/welfare spectrum and worked with restaurants to increase plant-based offerings. They practice a strategic agnosticism that small orgs could only dream of.

Recently, Anima spotted a long-shot opportunity to promote animal welfare values among AI companies in San Francisco, 6000 miles from their headquarters in Warsaw and even further outside their areas of expertise. That might not sound ideal, but the more I thought about it, the more I realized they're the perfect org to take this on. Since they don't need to fundraise for it specifically, they can be unflinchingly honest about whether or not it is working.

This strategic agnosticism doesn't always manifest in practice– large orgs can just as easily get complacent and stop experimenting altogether. And sometimes the fanatical determination of startups is key to unlocking real impact, spending years fighting tooth and nail where a larger org would have given up.

That kind of determination was crucial in my own experience building Pro-Animal Future. We knew it was ride or die, and there was no organizational safety net to catch us and reassign us to a different project if we failed. I'm glad we stuck with it and I think the result was good for animals, even though I know I'm still biased to think that way after moving on from the org.

### 3.1 Getting close to the final boss

In normal times, we need a mix of agility and focus. We don't always want everyone swerving all over the road chasing after the latest fad.

But this balance is changing. In the next few years, the value of strategic agility is going to skyrocket, because the pace of change is going to accelerate. It's like we're getting into the final levels of a game of Space Invaders. The aliens are speeding up every round. Long-term strategic planning will become less and less valuable, our impact more and more dominated by the speed of our reflexes.

So who will be better able to balance focus and agility: small orgs, or big orgs?

Two weeks ago, I would have confidently said small orgs. Now I think the answer is just: smart orgs.

Small orgs should stop thinking of themselves as _pursuing a specific intervention_ and start focusing on _building an agile organization_, a vehicle that can be steered to jump on the most important opportunities as they emerge.

Meanwhile, big orgs need to trim their bureaucracy. Even if leaders aren't interested in pushing AI adoption inside your org, any policies blocking your staff from experimenting are a massive own goal. It's time to cut the red tape.

Organizations that get stuck in strategic tunnel-vision or in outdated ways of working will fail, and deserve it. You might be able to keep getting donations, but you'll let more animals die.

How far would you go to achieve change for animals? Would you dedicate your life to the movement? Would you go to prison?

Would you start using Claude Cowork? Or have Claude teach you to configure OpenClaw on a remote virtual machine?

We're about to find out.

Build on,

Sandcastles

---

If you found this essay useful, the best way to help is to share it with another activist!

---

### Footnotes:

1. OpenAI is not known for elegant model names.

2. Industry-wide, the cost of API tokens has fallen by 90% in two years.

3. This is not speciesist, I have nothing but admiration for worms' ability to evade danger through sheer wriggling.

4. Mic drop aside, you should be very careful with OpenClaw's many security issues. I won't judge you for waiting a few more weeks for secure versions.
